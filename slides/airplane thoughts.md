Examples:

- cyclist crossing the Hawthorne  bridge each morning and noting:
  - rain
  - snow
  - cloudy
  - warm/cold
  - lots of bikes
  - lots of cars
  - bridge up
- Californian
  - smog level
  - email inbox full
  - web traffic
  - stock price
  - freeway jammed
  - dodgers win
- chatbot (you're moments take millisec onot your world is much smaller
  - "lol"
  - "great"
  - "what"
  - ":-)"
- Google translate & Facebook fasttext
  - "a"
  - "b"
  - "!"
  - "ñ"
  - "é"
- muellerbot
  - "Michael Cohen"
  - "IRA"
  - "Clinton"
  - "October 2016"

A deep learning brain is just noticing the presence or absence of phrases, words, half-words, or characters. Draw block diagram to make it clear that there's an add and multiply within reach neuron and that it's a bunch of weights, one for each word or word vector dimension.

Show diagram and results for character predictor with single neuron or unit, with one hot encoded input.
- linear
- linear with 2-char memory
- recurrent
- lstm
- transformer (GPT)
- BERT
- GPT-2


Show a single lstm character predictor.
